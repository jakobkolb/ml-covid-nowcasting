{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10821dad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import seaborn as sn\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from load_data import loading_and_pre_processing_pipeline\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "feature_data = (\n",
    "    loading_and_pre_processing_pipeline()\n",
    "    .sort_values(\"user_id\")\n",
    ")\n",
    "\n",
    "feature_data['date_diff'] = (feature_data['next_date'] - feature_data['date']).apply(lambda date: date.days)\n",
    "\n",
    "feature_data = feature_data.sort_values('date')\n",
    "feature_data = feature_data[feature_data['date_diff'] <= 30]\n",
    "\n",
    "print(\n",
    "    f'{len(feature_data)} records from {len(set(feature_data[\"user_id\"].values))} users present. '\n",
    ")\n",
    "print(f\"{len(feature_data.dropna(axis=0))} of which are complete.\")\n",
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021a88c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_auc_score\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from log_reg_model import create_model\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SPLITS = 10\n",
    "\n",
    "TARGET = 'test_result'\n",
    "labeled_data = feature_data[feature_data[TARGET].notna()].drop(columns=['next_test_result', 'test_week_start', 'next_week', 'next_date', 'date', 'user_id'])\n",
    "features = labeled_data.drop(columns=TARGET).columns\n",
    "\n",
    "# time series split for training and testing\n",
    "tsp = TimeSeriesSplit(n_splits=SPLITS)\n",
    "\n",
    "lr_pr_auc = []\n",
    "gluon_pr_auc = []\n",
    "lr_roc_auc = []\n",
    "gluon_roc_auc = []\n",
    "split_date = []\n",
    "test_data_start = []\n",
    "train_data_end = []\n",
    "\n",
    "for train_index, test_index in tqdm(tsp.split(labeled_data), total=SPLITS):\n",
    "\n",
    "    # save the start and end date of the train and test data\n",
    "    split_date.append(feature_data[feature_data[TARGET].notna()].iloc[train_index].date.max())\n",
    "    test_data_start.append(feature_data[feature_data[TARGET].notna()].iloc[test_index].date.min())\n",
    "    train_data_end.append(feature_data[feature_data[TARGET].notna()].iloc[train_index].date.max())\n",
    "\n",
    "    # train and test data\n",
    "    train_data = labeled_data.iloc[train_index]\n",
    "    test_data = labeled_data.iloc[test_index]\n",
    "\n",
    "    # under sampling to balance training data\n",
    "    X_train_resampled, y_train_resampled = RandomUnderSampler().fit_resample(train_data.values, train_data[[TARGET]].astype(bool).values)\n",
    "    train_data_resampled = pd.DataFrame(X_train_resampled, columns=test_data.columns)\n",
    "\n",
    "    # under sampling to balance test data\n",
    "    X_resampled, y_resampled = RandomUnderSampler().fit_resample(test_data.values, test_data[[TARGET]].astype(bool).values)\n",
    "    test_data_resampled = pd.DataFrame(X_resampled, columns=test_data.columns)\n",
    "\n",
    "    # training the logistic regression model\n",
    "    lr_model = create_model()\n",
    "    lr_model.fit(train_data[features], train_data[TARGET].astype(bool))\n",
    "\n",
    "    # training the gluon model\n",
    "    gluon_predictor = TabularPredictor(label=TARGET, eval_metric='balanced_accuracy', sample_weight='auto_weight').fit(train_data_resampled)\n",
    "\n",
    "    # calculate precision recall area under the curve and save to row in dataframe\n",
    "    def calculate_precision_recall_area_under_curve(model, data, target):\n",
    "        try:\n",
    "            proba = model.predict_proba(data[features])[:, 1]\n",
    "        except TypeError:\n",
    "            proba = model.predict_proba(data[features])[True].values\n",
    "        precision, recall, thresholds = precision_recall_curve(data[target].astype(bool), proba)\n",
    "        return auc(recall, precision)\n",
    "\n",
    "    # calculate roc area under the curve\n",
    "    def calculate_roc_area_under_curve(model, data, target):\n",
    "        try:\n",
    "            proba = model.predict_proba(data[features])[:, 1]\n",
    "        except TypeError:\n",
    "            proba = model.predict_proba(data[features])[True].values\n",
    "        return roc_auc_score(data[target].astype(bool), proba)\n",
    "\n",
    "    lr_pr_auc.append(calculate_precision_recall_area_under_curve(lr_model, test_data_resampled, TARGET))\n",
    "    gluon_pr_auc.append(calculate_precision_recall_area_under_curve(gluon_predictor, test_data_resampled, TARGET))\n",
    "\n",
    "    lr_roc_auc.append(calculate_roc_area_under_curve(lr_model, test_data_resampled, TARGET))\n",
    "    gluon_roc_auc.append(calculate_roc_area_under_curve(gluon_predictor, test_data_resampled, TARGET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc2dd3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot the auc curve for both models over time series splits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "pd.DataFrame({'Logistic regression': lr_pr_auc, 'Autogluon best model': gluon_pr_auc, 'date': split_date}).set_index('date').plot(ax=ax, kind='line')\n",
    "ax.set_title(\"Precision recall AUC for Logistic regression and Autogluon best model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc28ad",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Apparently, the Autogluon best model performs better than the Logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f272df",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "pd.DataFrame({'Logistic regression': lr_roc_auc, 'Autogluon best model': gluon_roc_auc, 'date': split_date}).set_index('date').plot(ax=ax, kind='line')\n",
    "ax.set_title(\"ROC AUC for Logistic regression and Autogluon best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224add1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gluon_predictor.feature_importance(test_data_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c52b60",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gluon_predictor.leaderboard(test_data_resampled, extra_metrics=['roc_auc'], silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11e78b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# calculate the confusion matrix and plot it\n",
    "\n",
    "def plot_confusion_matrix_on_axis(ax, y_true, y_pred):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    tick_marks = np.arange(len(set(y_true)))\n",
    "    ax.set_xticks(tick_marks, set(y_true), rotation=45)\n",
    "    ax.set_yticks(tick_marks, set(y_true))\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "    plt.tight_layout()\n",
    "    ax.grid(visible=None)\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "\n",
    "# plot roc curve on axis\n",
    "\n",
    "def plot_roc_curve_on_ax(ax, y_true, y_pred):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Receiver operating characteristic example')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "# plot precision recall curve on axis\n",
    "\n",
    "def plot_precision_recall_curve_on_ax(ax, y_true, y_pred):\n",
    "    from sklearn.metrics import precision_recall_curve, auc\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    ax.plot(recall, precision, label='Precision-Recall curve (AUC = %0.2f)' % pr_auc)\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_title('Precision-Recall example')\n",
    "    ax.legend(loc=\"lower left\")\n",
    "\n",
    "# plot feature importance on axis\n",
    "\n",
    "def plot_permutation_feature_importance(ax, target, feature_names, test_data, classifier):\n",
    "    y_test = test_data[target].astype(bool).values\n",
    "    X_test = test_data[feature_names]\n",
    "    try:\n",
    "        importance = permutation_importance(classifier, X_test, y_test, n_repeats=2, scoring=\"roc_auc\")\n",
    "        decreasing_importance = (\n",
    "            pd.DataFrame({\"mean\": importance[\"importances_mean\"], \"label\": feature_names})\n",
    "            .sort_values(\"mean\", ascending=False)\n",
    "            .label.values\n",
    "        )\n",
    "\n",
    "        df = (\n",
    "            pd.DataFrame(\n",
    "                columns=pd.Index(data=feature_names, name=\"features\"),\n",
    "                data=importance[\"importances\"].T,\n",
    "            )\n",
    "            .stack(\"features\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        df.columns = [\"iter\", \"features\", \"importance\"]\n",
    "        sns.boxplot(\n",
    "            y=\"features\",\n",
    "            x=\"importance\",\n",
    "            data=df,\n",
    "            order=decreasing_importance,\n",
    "            ax=ax,\n",
    "        )\n",
    "    except:\n",
    "        gluon_feature_importance = classifier.feature_importance(test_data)\n",
    "        columns = ['features'] + gluon_feature_importance.columns.tolist()\n",
    "        df = gluon_feature_importance.reset_index()\n",
    "        df.columns= columns\n",
    "        print(gluon_feature_importance)\n",
    "        sns.boxplot(\n",
    "            y=\"features\",\n",
    "            x=\"importance\",\n",
    "            data=df,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# plot the confusion matrix and roc curve for the autogluon best model\n",
    "def plot_analysis(axes: List, target: str, features: List[str], test_data: pd.DataFrame, predictor):\n",
    "    try:\n",
    "        y_pred = predictor.predict(test_data[features])\n",
    "        y_proba = predictor.predict_proba(test_data[features])[:, 1]\n",
    "    except TypeError:\n",
    "        y_pred = predictor.predict(test_data[features])\n",
    "        y_proba = predictor.predict_proba(test_data[features])[True].values\n",
    "    y_true = test_data[target].astype(bool)\n",
    "    plot_confusion_matrix_on_axis(axes[0], y_true, y_pred)\n",
    "    plot_roc_curve_on_ax(axes[1], y_true, y_proba)\n",
    "    plot_precision_recall_curve_on_ax(axes[2], y_true, y_proba)\n",
    "    plot_permutation_feature_importance(axes[3], target, features, test_data, predictor)\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(10, 20))\n",
    "\n",
    "lr_axes = axes[:,1]\n",
    "plot_analysis(lr_axes, TARGET, features, test_data_resampled, lr_model)\n",
    "\n",
    "gluon_axes = axes[:,0]\n",
    "plot_analysis(gluon_axes, TARGET, features, test_data_resampled, gluon_predictor)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a846e88a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
